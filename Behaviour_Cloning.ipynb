{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I have used Jupyter notebook to enable ease of evaluation and ensure that everyone can run it using Google Colab GPU reseources. \n",
        "This line mounts my Google drive where the data is stored\n"
      ],
      "metadata": {
        "id": "iyenGbxRnJvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue01LZtF-SEA",
        "outputId": "6e16c7cd-dc9e-465d-90e3-afa7e4a26e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After mounting my Google drive, I have copied the given data file and unzipped it. \n",
        "The user has to change the path to the data folder in their drive"
      ],
      "metadata": {
        "id": "OjBv0BVNmvWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/ml-engineer-testing-task-data.zip\" \"/content/data.zip\" \n",
        "!unzip \"/content/data.zip\""
      ],
      "metadata": {
        "id": "H6A3L7pN-kyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the Arcade Learning Environment(ALE) framework for emulating Breakout from the Atari 2600 games. The Open AI gym is built on top of ALE thus offers native support to ALE.  For more details about the framework please refer to - https://github.com/mgbellemare/Arcade-Learning-Environment "
      ],
      "metadata": {
        "id": "WQLWf0KDoFMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py"
      ],
      "metadata": {
        "id": "HTOYY-O1g_gZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9d8b91-b8af-49be-dbf8-9c0413fa5ded"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ale-py\n",
            "  Downloading ale_py-0.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from ale-py) (4.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ale-py) (1.19.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.6.0)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the ROM files required for ATARI 2600 games. The ROM files have been downloaded from http://www.atarimania.com.  However, ALE does not support all ROM files. The code snippet imports the supported ROMS and enables us to easily import the required ROMs using the import command. "
      ],
      "metadata": {
        "id": "bkdpJYo_vPns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!unzip \"./rars/ROMS.zip\"\n",
        "!mkdir \"./checkpoints\"\n",
        "!ale-import-roms ROMS"
      ],
      "metadata": {
        "id": "nPx5A89EpJrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Python libraries. The code uses the OpenCV library to process images. Please refer to -https://github.com/opencv/opencv/releases for additional information about OpenCV"
      ],
      "metadata": {
        "id": "8f0GS1GlpOl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from collections import namedtuple, deque\n",
        "import cv2\n",
        "import random\n",
        "from ale_py import ALEInterface\n",
        "from copy import deepcopy\n"
      ],
      "metadata": {
        "id": "prjqbNm2hqKu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Expert class loads the episode start, actions, observations and rewards of the Expert provided in the data.  "
      ],
      "metadata": {
        "id": "g_1dHkbppybJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class expert():\n",
        "  def __init__(self, expert_path) -> None:\n",
        "      if  expert_path is not None:\n",
        "          assert os.path.exists(expert_path)\n",
        "      actions = np.load(expert_path+\"actions.npy\") \n",
        "      # Aligning the provided actions to the actions in ALE\n",
        "      self.actions = np.where(actions<2,actions,actions+1)\n",
        "      self.obs= np.load(expert_path+\"obs.npy\")\n",
        "      self.rewards=np.load(expert_path+\"rewards.npy\")\n",
        "      self.episode_starts=np.load(expert_path+\"episode_starts.npy\")"
      ],
      "metadata": {
        "id": "H4-pdI5rAJW9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below section has some util functions which help streamline the code and make it easy to read and debug. The functions help the code save and load the checkpoints. It also helps in formatting the tensor. There is also a function which enables the agent perfrom null runs in the environment during evaluation. "
      ],
      "metadata": {
        "id": "gn1gEuRxqEJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, checkpoint_dir):\n",
        "\tfilename = checkpoint_dir + '/network.pth.tar'\n",
        "\tprint(\"Saving checkpoint at \" + filename + \" ...\")\n",
        "\ttorch.save(state, filename)  # save checkpoint\n",
        "\tprint(\"Saved checkpoint.\")\n",
        "\n",
        "def get_checkpoint(checkpoint_dir):\n",
        "\tresume_weights = checkpoint_dir + '/network.pth.tar'\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tprint(\"Attempting to load Cuda weights...\")\n",
        "\t\tcheckpoint = torch.load(resume_weights)\n",
        "\t\t#print \"Loaded weights.\"\n",
        "\telse:\n",
        "\t\tprint(\"Attempting to load weights for CPU...\")\n",
        "\t\t# Load GPU model on CPU\n",
        "\t\tcheckpoint = torch.load(resume_weights,\n",
        "\t\t\t\t\t\t\t\tmap_location=lambda storage,\n",
        "\t\t\t\t\t\t\t\tloc: storage)\n",
        "\t\tprint(\"Loaded weights.\")\n",
        "\treturn checkpoint\n",
        "\n",
        "def long_tensor(input):\n",
        "\tif torch.cuda.is_available():\n",
        "\t\treturn torch.cuda.LongTensor(input)\n",
        "\telse:\n",
        "\t\treturn torch.LongTensor(input)\n",
        "\n",
        "def float_tensor(input):\n",
        "\tif torch.cuda.is_available():\n",
        "\t\treturn torch.cuda.FloatTensor(input)\n",
        "\telse:\n",
        "\t\treturn torch.FloatTensor(input)\n",
        "\n",
        "def perform_no_ops(ale, no_op_max, preprocessor, state):\n",
        "\tnum_no_ops = np.random.randint(1, no_op_max + 1)\n",
        "\tfor _ in range(num_no_ops):\n",
        "\t\tale.act(0)\n",
        "\t\tpreprocessor.add(ale.getScreenRGB())\n",
        "\t\n",
        "\twhile len(preprocessor.preprocess_stack) < 4:\n",
        "\t\tale.act(0)\n",
        "\t\tpreprocessor.add(ale.getScreenRGB())\n",
        "\t\n",
        "\tstate.add_frame(preprocessor.preprocess())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hRFTQUX7gpnf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Preprocesser creates a stack of 4 frames. It then throws away the first two and takes an element wise maximum of the other two. Then it resizes the image into 84,84 to ensure that the image is aligned with the data provided in the example. \n",
        "\n",
        "Note: The pdf document mentions that the provided images are grayscale. However, the images have 3 RGB channel. Thus, this code provides the flexibility to process both coloured and grayscale input images.\n",
        "\n"
      ],
      "metadata": {
        "id": "OMldDCHcwGY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "class Preprocessor:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.preprocess_stack = deque([], 4)\n",
        "\n",
        "\tdef add(self, aleRGB):\n",
        "\t\tself.preprocess_stack.append(aleRGB)\n",
        "\n",
        "\t'''\n",
        "\tAs mentioned in the question, preprocessing takes the maximum pixel\n",
        "\tvalues of 4 consecutive frames. It then\n",
        "\tgrayscales the image, and resizes it to 84x84.\n",
        "\t'''\n",
        "\tdef preprocess(self):\n",
        "\t\tassert len(self.preprocess_stack) == 4\n",
        "\t\tmax_stack = np.maximum(self.preprocess_stack[2], self.preprocess_stack[3])\n",
        "\t\tif sequential:\n",
        "\t\t\timg=self.resize(self.grayscale(max_stack))\n",
        "\t\t\treturn img\n",
        "\t\telse:\n",
        "\t\t  return self.resize(max_stack)\n",
        "\n",
        "\t'''\n",
        "\tTakes in an RGB image and returns a grayscaled\n",
        "\timage.\n",
        "\t'''\n",
        "\tdef grayscale(self, img):\n",
        "\t\treturn cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "\t'''\n",
        "\tResizes the input to an 84x84 image.\n",
        "\t'''\n",
        "\tdef resize(self, image):\n",
        "\t\treturn cv2.resize(image, (84, 84),interpolation=cv2.INTER_LINEAR )"
      ],
      "metadata": {
        "id": "CZpaCDOFkgKG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class is a wrapper for the the Arcade learning environment and defines methods to fetch the observations, take actions and reset the game. It also returns a terminal state once the game is over."
      ],
      "metadata": {
        "id": "l8T3KEIPxEwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ALEInterfaceWrapper:\n",
        "\tdef __init__(self, repeat_action_probability):\n",
        "\t\tself.internal_action_repeat_prob = repeat_action_probability\n",
        "\t\tself.prev_action = 0\n",
        "\t\tself.ale = ALEInterface()\n",
        "\t\t'''\n",
        "\t\tThis sets the probability from the default 0.25 to 0.\n",
        "\t\tIt ensures deterministic actions.\n",
        "\t\t'''\n",
        "\t\tself.ale.setFloat('repeat_action_probability', repeat_action_probability)\n",
        "\n",
        "\tdef getScreenRGB(self):\n",
        "\t\treturn self.ale.getScreenRGB()\n",
        "\t\t\n",
        "\tdef game_over(self):\n",
        "\t\treturn self.ale.game_over()\n",
        "\n",
        "\tdef reset_game(self):\n",
        "\t\tself.ale.reset_game()\n",
        "\n",
        "\tdef lives(self):\n",
        "\t\treturn self.ale.lives()\n",
        "\n",
        "\tdef getMinimalActionSet(self):\n",
        "\t\treturn self.ale.getMinimalActionSet()\n",
        "\n",
        "\tdef setInt(self, key, value):\n",
        "\t\tself.ale.setInt(key, value)\n",
        "\n",
        "\tdef setFloat(self, key, value):\n",
        "\t\tself.ale.setFloat(key, value)\n",
        "\n",
        "\tdef loadROM(self, rom):\n",
        "\t\tself.ale.loadROM(rom)\n",
        "\n",
        "\tdef act(self, action):\n",
        "\t\tactual_action = action\n",
        "\t\treturn self.ale.act(actual_action)"
      ],
      "metadata": {
        "id": "nOw5FDoZfQTH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class takes the observation from ALE and provides the flexibility to process a sequence of images. This enables the algorithm to create a sequence of output."
      ],
      "metadata": {
        "id": "t_TwIIjqyQBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State:\t\n",
        "    def __init__(self, hist_len):\n",
        "      # Initialize a 1 x hist_len x 84 x 84  state\n",
        "      self.hist_len = hist_len\n",
        "      if sequential:\n",
        "        self.state = np.zeros((1, hist_len, 84, 84), dtype=np.float32)\n",
        "        self.insertLoc = 0\n",
        "      else:\n",
        "        self.state = np.zeros((1,84, 84,3), dtype=np.float32)\n",
        "      \n",
        "\n",
        "    def add_frame(self, img):\n",
        "      if sequential:\n",
        "        self.state[0, self.insertLoc, ...] = img.astype(np.float32)/255.0\n",
        "        self.insertLoc = (self.insertLoc + 1) % self.hist_len\n",
        "      else:\n",
        "        self.state = img.astype(np.float32)\n",
        "\n",
        "    def get_state(self):      \n",
        "      if len(self.state.shape)!= 4:\n",
        "        self.state= np.expand_dims(self.state,axis=0)\n",
        "      if sequential: \n",
        "         return np.roll(self.state, 0 - self.insertLoc, axis=1)\n",
        "      return self.state\n",
        "     "
      ],
      "metadata": {
        "id": "LUYUoD-ak592"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class creates the trainnig dataset based on the Expert's trajectory. The add frame method enables us to add each state action pair sequentially as per the required dataset size. The dataset size enables the user to control the amount of training data as per the compute.  "
      ],
      "metadata": {
        "id": "J3m2s02fy69L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Example = namedtuple('Example', 'state action')\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "\tdef __init__(self, size, hist_len):\n",
        "\t\tself.size = size\n",
        "\t\tself.hist_len = hist_len\n",
        "\t\tif sequential:\n",
        "\t\t\tself.states = np.empty((size, hist_len, 84, 84), dtype=np.float32)\n",
        "\t\telse:\n",
        "\t\t\tself.states = np.empty((size, 84, 84,3), dtype=np.float32)\n",
        "\t\tself.actions = np.empty(size, dtype=np.uint8)\n",
        "\t\tself.index = 0\n",
        "\t\tself.sample_indices = range(size)\n",
        "\t\tself.shuffle_indices()\n",
        "\t\tself.minibatch_index = 0\n",
        "\n",
        "\tdef shuffle_indices(self):\n",
        "\t\trandom.shuffle(list(self.sample_indices))\n",
        "\n",
        "\tdef clear(self):\n",
        "\t\tself.actions = np.empty(self.size, dtype=np.uint8)\n",
        "\t\tif sequential:\n",
        "\t\t\tself.states = np.empty((self.size,self.hist_len, 84, 84), dtype=np.uint8)\n",
        "\t\telse:\n",
        "\t\t\tself.states = np.empty((self.size, 84, 84,3), dtype=np.uint8)\n",
        "\t\t\n",
        "\tdef grayscale(self, img):\n",
        "\t\treturn cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "\tdef add_item(self, state, action, episode_starts):\n",
        "\t\tif self.index == self.size:\n",
        "\t\t\traise ValueError(\"Dataset is full. Clear dataset before adding anything.\")\n",
        "\t \n",
        "\t\tpath = \"/content/ml-engineer-testing-task/data/\"\n",
        "\t\tif episode_starts:\n",
        "\t\t\t\tself.states[0, ...] = self.grayscale(cv2.imread(os.path.join(path,state)))\n",
        "\t\tself.states[self.index, ...] = self.grayscale(cv2.imread(os.path.join(path,state)))\n",
        "\t\tself.actions[self.index] = action\n",
        "\t\tself.index += 1\n",
        "\n",
        "\tdef sample_minibatch(self, batch_size):\n",
        "\t\tbatch = []\n",
        "\t\tfor _ in range(batch_size):\n",
        "\t\t\tindex = self.sample_indices[self.minibatch_index]\n",
        "\t\t\tbatch.append(Example(state=self.states[index],action=self.actions[index]))\n",
        "\t\t\tself.minibatch_index = self.minibatch_index + 1\n",
        "\t\t\tif self.minibatch_index >= self.size:\n",
        "\t\t\t\tself.minibatch_index = 0\n",
        "\t\t\t\tself.shuffle_indices()\n",
        "\t\treturn batch"
      ],
      "metadata": {
        "id": "ffIccWmTiapw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class defines the Convolutional Neural Network(CNN) which is trained to learn from the expert's trajectory. "
      ],
      "metadata": {
        "id": "nGMd6Oq6ztTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "\tdef __init__(self, num_output_actions):\n",
        "\t\tsuper(Network, self).__init__()\n",
        "\t\tif not sequential:\n",
        "\t\t\t\tself.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
        "\t\t\t\tnn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
        "\t\telse: \n",
        "\t\t\t\tself.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
        "\t\t\t\tnn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
        "\t\tself.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "\t\tnn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
        "\t\tself.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\t\tnn.init.kaiming_normal_(self.conv3.weight, nonlinearity='relu')\n",
        "\t\tself.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
        "\t\tself.output = nn.Linear(512, num_output_actions)\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tif not sequential:\n",
        "\t\t\tinput= input.permute(0,3,1,2)\n",
        "\t\t\tinput =input.to(memory_format=torch.channels_last)\n",
        "\t\tconv1_output = F.relu(self.conv1(input))\n",
        "\t\tconv2_output = F.relu(self.conv2(conv1_output))\n",
        "\t\tconv3_output = F.relu(self.conv3(conv2_output)).contiguous()\n",
        "\t\tfc1_output = F.relu(self.fc1(conv3_output.view(conv3_output.size(0), -1)))\t\n",
        "\t\toutput = self.output(fc1_output)\n",
        "\t\treturn conv1_output, conv2_output, conv3_output, fc1_output, output"
      ],
      "metadata": {
        "id": "NUW4vWwwhf7Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the central class which enables the agent to learn from the demonstration provided by the expert. I have used the Behaviour Cloning imitation learning technique to make the agent learn from the expert."
      ],
      "metadata": {
        "id": "mX-TpMYyzrO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Imitator:\n",
        "\tdef __init__(self, min_action_set,\n",
        "\t\t\t\tlearning_rate,\n",
        "\t\t\t\talpha,\n",
        "\t\t\t\tmin_squared_gradient,\n",
        "\t\t\t\tcheckpoint_dir,\n",
        "\t\t\t\thist_len,\n",
        "\t\t\t\tl2_penalty):\n",
        "\t\tself.minimal_action_set = min_action_set\n",
        "\t\tself.network = Network(len(self.minimal_action_set))\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\tprint(\"Initializing Cuda Nets...\")\n",
        "\t\t\tself.network.cuda()\n",
        "\t\tself.optimizer = optim.Adam(self.network.parameters(),\n",
        "\t\tlr=learning_rate, weight_decay=l2_penalty)\n",
        "\t\tself.checkpoint_directory = checkpoint_dir\n",
        "\t\tself.losses = []\n",
        "\t\tself.accs=[]\n",
        "\n",
        "\n",
        "\tdef predict(self, state):\n",
        "\t\t# predict action probabilities\n",
        "\t\toutputs = self.network(Variable(float_tensor(state)))\n",
        "\t\tvals = outputs[len(outputs) - 1].data.cpu().numpy()\n",
        "\t\treturn vals\n",
        "\n",
        "\tdef get_action(self, state):\n",
        "\t\tvals = self.predict(state)\n",
        "\t\treturn self.minimal_action_set[np.argmax(vals)]\n",
        "\n",
        "\t\n",
        "\tdef compute_labels(self, sample, minibatch_size):\n",
        "\t\tlabels = Variable(long_tensor(minibatch_size))\n",
        "\t\tactions_taken = [x.action for x in sample]\n",
        "\t\taction_indices = [self.minimal_action_set.index(x) for x in actions_taken]\n",
        "\t\tfor index in range(len(action_indices)):\n",
        "\t\t\tlabels[index] = action_indices[index]\n",
        "\t\treturn labels\n",
        "\n",
        "\tdef get_loss(self, outputs, labels):\n",
        "\t\treturn nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "\tdef train(self, dataset, minibatch_size):\n",
        "\t\tsample = dataset.sample_minibatch(minibatch_size)\n",
        "\t\tstate = Variable(float_tensor(np.stack([np.squeeze(x.state) for x in sample])))\n",
        "\t\tlabels = self.compute_labels(sample, minibatch_size)\n",
        "\t\tself.optimizer.zero_grad()\n",
        "\t\tactivations = self.network(state)\n",
        "\t\toutput = activations[len(activations) - 1]\n",
        "\t\tloss = self.get_loss(output, labels)\n",
        "\t\tself.losses.append(loss)\n",
        "\t\tloss.backward()\n",
        "\t\tself.optimizer.step()\n",
        "\t\tcorrect = (torch.argmax(output, dim=1) == labels).float().sum()\n",
        "\t\tacc = correct/output.shape[0]\n",
        "\t\tself.accs.append(acc)\n",
        "\n",
        "\n",
        "\tdef checkpoint_network(self):\n",
        "\t\tprint(\"Checkpointing Weights\")\n",
        "\t\tsave_checkpoint({'state_dict': self.network.state_dict()}, self.checkpoint_directory)\n",
        "\t\tprint(\"Checkpointed.\")\n",
        "\t"
      ],
      "metadata": {
        "id": "7oPjG_G9fR7O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class implements methods to evaluate the trained DQN agent. The perfromance is measured for 100 episodes. The agent uses an epsilon delta method to ensure optimum balance between exploration and exploitation. "
      ],
      "metadata": {
        "id": "d9jtzvll1SQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "\n",
        "\tdef __init__(self, rom, cap_eval_episodes=True, time_limit=60 * 60 * 30 / 4,\n",
        "\t\t\t\taction_repeat=4, hist_len=4, ale_seed=100, action_repeat_prob=0,\n",
        "\t\t\t\tnum_eval_episodes=100):\n",
        "\t\tself.cap_eval_episodes = cap_eval_episodes\n",
        "\t\tself.time_limit = time_limit\n",
        "\t\tself.action_repeat = action_repeat\n",
        "\t\tself.hist_len = hist_len\n",
        "\t\tself.rom = rom\n",
        "\t\tself.ale_seed = ale_seed\n",
        "\t\tself.action_repeat_prob = action_repeat_prob\n",
        "\t\tself.num_eval_episodes = num_eval_episodes\n",
        "\n",
        "\tdef evaluate(self, agent):\n",
        "\t\tale = self.setup_eval_env(self.ale_seed, self.action_repeat_prob, self.rom)\n",
        "\t\tself.eval(ale, agent)\n",
        "\n",
        "\tdef setup_eval_env(self, ale_seed, action_repeat_prob, rom):\n",
        "\t\tale = ALEInterfaceWrapper(action_repeat_prob)\n",
        "\t\t#Set the random seed for the ALE\n",
        "\n",
        "\t\tale.setInt('random_seed', ale_seed)\n",
        "\t\t'''\n",
        "\t\tThis sets the probability from the default 0.25 to 0.\n",
        "\t\tIt ensures deterministic actions.\n",
        "\t\t'''\n",
        "\t\tale.setFloat('repeat_action_probability', action_repeat_prob)\n",
        "\t\t# Load the ROM file\n",
        "\t\tale.loadROM(rom)\n",
        "\t\treturn ale\n",
        "\n",
        "\tdef eval(self, ale, agent):\n",
        "\t\taction_set = ale.getMinimalActionSet()\n",
        "\t\trewards = []\n",
        "\t\tfor i in range(self.num_eval_episodes):\n",
        "\t\t\tale.reset_game()\n",
        "\t\t\tpreprocessor = Preprocessor()\n",
        "\t\t\tstate = State(self.hist_len)\n",
        "\t\t\tsteps = 0\n",
        "\t\t\tperform_no_ops(ale, 10, preprocessor, state)\n",
        "\t\t\tepisode_reward = 0\n",
        "\t\t\twhile not ale.game_over() and steps < self.time_limit:\n",
        "\t\t\t\tif np.random.uniform() < 0.1:\n",
        "\t\t\t\t\taction = np.random.choice(action_set)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\taction = agent.get_action(state.get_state())\n",
        "\t\t\t\t\tif DEBUG:\n",
        "\t\t\t\t\t\tprint(\"action\", action)\n",
        "\t\t\t\tfor _ in range(self.action_repeat):\n",
        "\t\t\t\t\tepisode_reward += ale.act(action)\n",
        "\t\t\t\t\tpreprocessor.add(ale.getScreenRGB())\t\t \t\t\n",
        "\t\t\t\tstate.add_frame(preprocessor.preprocess())\n",
        "\t\t\t\tsteps += 1\n",
        "\t\t\tprint(\"Episode \" + str(i) + \" reward is \" + str(episode_reward))\n",
        "\t\t\trewards.append(episode_reward)\n",
        "\t\tprint(\"Mean reward is: \" + str(np.mean(rewards)))"
      ],
      "metadata": {
        "id": "fSKhQRqsfu6Q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main body of the code which trains the DQN agent as per the hyperparameters provided by the user. The most optimum hyperparameters as per my tuning is given below. First the ALE environement is initialised, then the agent is trained to learn from the demonstration from the expert. Finally, the perfromance of the agent is evaluated in the Breakout game. "
      ],
      "metadata": {
        "id": "2HLP-gBa2Zew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pdb import set_trace\n",
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('bmh')\n",
        "\n",
        "\n",
        "def smooth(losses, run=10):\n",
        "\tnew_losses = []\n",
        "\tfor i in range(len(losses)):\n",
        "\t\tnew_losses.append(np.mean(losses[max(0, i - 10):i+1]))\n",
        "\treturn new_losses\n",
        "\n",
        "def plot(metric, checkpoint_dir, name):\n",
        "\t\t#p=plt.plot(smooth(losses, 25))\n",
        "\t\tfig=plt.figure()\n",
        "\t\tp=plt.plot(metric)\n",
        "\t\tplt.xlabel(\"Update\")\n",
        "\t\tplt.ylabel(f\"{name}\")\n",
        "\t\tplt.legend(loc='lower center')\n",
        "\t\tplt.savefig(os.path.join(checkpoint_dir, f\"{name}.png\"))\n",
        "\t\tplt.close(fig)\n",
        "\t\n",
        "def train(rom,\n",
        "\t\tale_seed,\n",
        "\t\taction_repeat_probability,\n",
        "\t\tlearning_rate,\n",
        "\t\talpha,\n",
        "\t\tmin_squared_gradient,\n",
        "\t\tl2_penalty,\n",
        "\t\tminibatch_size, \n",
        "\t\thist_len,\n",
        "\t\tdiscount,\n",
        "\t\tcheckpoint_dir,\n",
        "\t\tupdates,\n",
        "\t\tdataset):\n",
        "\n",
        "\n",
        "\tale = ALEInterfaceWrapper(action_repeat_probability)\n",
        "\n",
        "\t#Set the random seed for the ALE\n",
        "\tale.setInt('random_seed', ale_seed)\n",
        "\n",
        "\t# Load the ROM file\n",
        "\tale.loadROM(rom)\n",
        "\n",
        "\tprint(\"Minimal Action set is:\")\n",
        "\tprint(ale.getMinimalActionSet())\n",
        "\n",
        "\n",
        "\t# create DQN agent\n",
        "\tagent = Imitator(ale.getMinimalActionSet(),\n",
        "\t\t\t\tlearning_rate,\n",
        "\t\t\t\talpha,\n",
        "\t\t\t\tmin_squared_gradient,\n",
        "\t\t\t\tcheckpoint_dir,\n",
        "\t\t\t\thist_len,\n",
        "\t\t\t\tl2_penalty)\n",
        "\n",
        "\tprint(\"Beginning training...\")\n",
        "\tlog_frequency = 1000\n",
        "\tlog_num = log_frequency\n",
        "\tupdate = 1\n",
        "\twhile update < updates:\n",
        "\t\tif update > log_num:\n",
        "\t\t\tprint(str(update) + \" updates completed.\")\n",
        "\t\t\tif DEBUG:\n",
        "\t\t\t\tprint(\"Loss: {:.3f}..\".format(agent.losses[-1].data.cpu().numpy()), \"Accuracy: {:.3f}\".format(agent.accs[-1].cpu().numpy()))\n",
        "\t\t\tlog_num += log_frequency\n",
        "\t\tagent.train(dataset, 32)\n",
        "\t\tupdate += 1\n",
        "\tprint(\"Training completed.\")\n",
        "\tagent.checkpoint_network()\n",
        "\tlosses_list,accs_list = [],[]\n",
        "\tfor loss,acc in zip(agent.losses,agent.accs):\n",
        "\t\tlosses_list.append(loss.data.cpu().numpy())\n",
        "\t\taccs_list.append(acc.cpu().numpy())\n",
        "\tplot(metric=losses_list, checkpoint_dir=checkpoint_dir,name=\"Loss\")\n",
        "\tplot(metric=accs_list, checkpoint_dir=checkpoint_dir,name=\"Accuracy\")\n",
        "\t#Evaluation\n",
        "\tevaluator = Evaluator(rom=rom)\n",
        "\tevaluator.evaluate(agent)"
      ],
      "metadata": {
        "id": "1Ql3pE_hTQ8B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arcade learning environment provides an easy to use method to import the ROMs."
      ],
      "metadata": {
        "id": "soQKX74VVlKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ale_py.roms import Breakout"
      ],
      "metadata": {
        "id": "MOTG0FTASlFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialising all the Hyperparameters. The hyperparameters can be passed using the Argument parser if running the code outside of Colab. However, Colab does not permit the use of argument parser. \n",
        "The *DEBUG* flag allows the user to print output at critical junctures of the code which makes it easy to debug the code. \n",
        "The *SEQUENTIAL*  flag provides the flexibility to the user to run the framework using the RGB channels without converting the image to Grayscale. The user should put SEQUENTIAL to **FALSE** to run the version with the RGB channels. The usual scenario for process a sequence of 4 frames converted to a grayscale image requires the SEQUENTIAL flag to be set to **TRUE.**"
      ],
      "metadata": {
        "id": "oR_5DMhkVwla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequential =True \n",
        "DEBUG = False\n",
        "rom = Breakout\n",
        "ale_seed =123\n",
        "action_repeat_probability =0.0\n",
        "dataset_size=20000\n",
        "updates =10000\n",
        "minibatch_size = 32\n",
        "hist_len=4\n",
        "discount=0.99\n",
        "learning_rate=0.01\n",
        "alpha=0.95\n",
        "min_squared_gradient=0.01\n",
        "l2_penalty= 0.001\n",
        "checkpoint_dir=\"checkpoints\""
      ],
      "metadata": {
        "id": "9J8aC3yllgD3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Expert demonstration using the Expert class. Then, using the dataset class to load all the demonstrations for further processing. "
      ],
      "metadata": {
        "id": "ZSpVp-wYVv1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RL_expert = expert(\"/content/ml-engineer-testing-task/data/\")\n",
        "data = Dataset(dataset_size, hist_len)\n",
        "episode_index_counter = 0\n",
        "for index in range(dataset_size):\n",
        "      state = str(RL_expert.obs[index])\n",
        "      action = RL_expert.actions[index]\n",
        "      data.add_item(state, action ,episode_starts=RL_expert.episode_starts[index])\n",
        "\n",
        "\n",
        "if DEBUG:\n",
        "    path = \"/content/ml-engineer-testing-task/data/\"\n",
        "    state = RL_expert.obs[2]\n",
        "    print(np.max(RL_expert.actions))\n",
        "    print(np.mean(RL_expert.rewards))\n",
        "    test_img= cv2.imread(os.path.join(path,state))\n",
        "    h,w,c = test_img.shape\n",
        "    print(f\"height{h} width{w} channel{c}\")\n",
        "    b,g,r = cv2.split(test_img)\n",
        "    frame_rgb = cv2.merge((r,g,b))\n",
        "    %matplotlib inline\n",
        "    plt.imshow(frame_rgb)"
      ],
      "metadata": {
        "id": "i5MicNPI6MhJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training method to train the DQN agent to learn from the expert class and then evaluate the performance of the DQN Agent."
      ],
      "metadata": {
        "id": "KTWUWmYRWh1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(rom,\n",
        "\t\tale_seed,\n",
        "\t\taction_repeat_probability,\n",
        "\t\tlearning_rate,\n",
        "\t\talpha,\n",
        "\t\tmin_squared_gradient,\n",
        "\t\tl2_penalty,\n",
        "\t\tminibatch_size, \n",
        "\t\thist_len,\n",
        "\t\tdiscount,\n",
        "\t\tcheckpoint_dir,\n",
        "\t\tupdates,\n",
        "\t\tdata)"
      ],
      "metadata": {
        "id": "FXYHbPthih6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ad5a32-5efd-4518-fd76-16afbe0f2268"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimal Action set is:\n",
            "[<Action.NOOP: 0>, <Action.FIRE: 1>, <Action.RIGHT: 3>, <Action.LEFT: 4>]\n",
            "Initializing Cuda Nets...\n",
            "Beginning training...\n",
            "1001 updates completed.\n",
            "2001 updates completed.\n",
            "3001 updates completed.\n",
            "4001 updates completed.\n",
            "5001 updates completed.\n",
            "6001 updates completed.\n",
            "7001 updates completed.\n",
            "8001 updates completed.\n",
            "9001 updates completed.\n",
            "Training completed.\n",
            "Checkpointing Weights\n",
            "Saving checkpoint at checkpoints/network.pth.tar ...\n",
            "Saved checkpoint.\n",
            "Checkpointed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n",
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 reward is 11\n",
            "Episode 1 reward is 11\n",
            "Episode 2 reward is 11\n",
            "Episode 3 reward is 0\n",
            "Episode 4 reward is 4\n",
            "Episode 5 reward is 8\n",
            "Episode 6 reward is 0\n",
            "Episode 7 reward is 0\n",
            "Episode 8 reward is 12\n",
            "Episode 9 reward is 11\n",
            "Episode 10 reward is 8\n",
            "Episode 11 reward is 0\n",
            "Episode 12 reward is 11\n",
            "Episode 13 reward is 0\n",
            "Episode 14 reward is 11\n",
            "Episode 15 reward is 11\n",
            "Episode 16 reward is 11\n",
            "Episode 17 reward is 0\n",
            "Episode 18 reward is 11\n",
            "Episode 19 reward is 11\n",
            "Episode 20 reward is 11\n",
            "Episode 21 reward is 8\n",
            "Episode 22 reward is 11\n",
            "Episode 23 reward is 0\n",
            "Episode 24 reward is 0\n",
            "Episode 25 reward is 0\n",
            "Episode 26 reward is 11\n",
            "Episode 27 reward is 0\n",
            "Episode 28 reward is 0\n",
            "Episode 29 reward is 11\n",
            "Episode 30 reward is 0\n",
            "Episode 31 reward is 11\n",
            "Episode 32 reward is 11\n",
            "Episode 33 reward is 0\n",
            "Episode 34 reward is 8\n",
            "Episode 35 reward is 0\n",
            "Episode 36 reward is 11\n",
            "Episode 37 reward is 11\n",
            "Episode 38 reward is 11\n",
            "Episode 39 reward is 0\n",
            "Episode 40 reward is 11\n",
            "Episode 41 reward is 11\n",
            "Episode 42 reward is 0\n",
            "Episode 43 reward is 11\n",
            "Episode 44 reward is 0\n",
            "Episode 45 reward is 0\n",
            "Episode 46 reward is 0\n",
            "Episode 47 reward is 11\n",
            "Episode 48 reward is 11\n",
            "Episode 49 reward is 0\n",
            "Episode 50 reward is 11\n",
            "Episode 51 reward is 7\n",
            "Episode 52 reward is 11\n",
            "Episode 53 reward is 0\n",
            "Episode 54 reward is 11\n",
            "Episode 55 reward is 9\n",
            "Episode 56 reward is 0\n",
            "Episode 57 reward is 0\n",
            "Episode 58 reward is 0\n",
            "Episode 59 reward is 0\n",
            "Episode 60 reward is 0\n",
            "Episode 61 reward is 11\n",
            "Episode 62 reward is 11\n",
            "Episode 63 reward is 0\n",
            "Episode 64 reward is 0\n",
            "Episode 65 reward is 11\n",
            "Episode 66 reward is 11\n",
            "Episode 67 reward is 11\n",
            "Episode 68 reward is 0\n",
            "Episode 69 reward is 11\n",
            "Episode 70 reward is 11\n",
            "Episode 71 reward is 0\n",
            "Episode 72 reward is 0\n",
            "Episode 73 reward is 11\n",
            "Episode 74 reward is 11\n",
            "Episode 75 reward is 11\n",
            "Episode 76 reward is 11\n",
            "Episode 77 reward is 11\n",
            "Episode 78 reward is 0\n",
            "Episode 79 reward is 0\n",
            "Episode 80 reward is 11\n",
            "Episode 81 reward is 0\n",
            "Episode 82 reward is 0\n",
            "Episode 83 reward is 11\n",
            "Episode 84 reward is 11\n",
            "Episode 85 reward is 0\n",
            "Episode 86 reward is 11\n",
            "Episode 87 reward is 11\n",
            "Episode 88 reward is 11\n",
            "Episode 89 reward is 0\n",
            "Episode 90 reward is 11\n",
            "Episode 91 reward is 0\n",
            "Episode 92 reward is 0\n",
            "Episode 93 reward is 9\n",
            "Episode 94 reward is 11\n",
            "Episode 95 reward is 0\n",
            "Episode 96 reward is 11\n",
            "Episode 97 reward is 0\n",
            "Episode 98 reward is 0\n",
            "Episode 99 reward is 0\n",
            "Mean reward is: 6.01\n"
          ]
        }
      ]
    }
  ]
}